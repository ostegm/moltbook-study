{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moltbook Blog Post: Reproducible Analysis\n",
    "\n",
    "**Data sources:**\n",
    "- `classified_posts.jsonl` — 56,700 classified posts (agents with 5+ posts)\n",
    "- `dataset_stats.json` — summary stats from the full 86,823-post dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import json\nimport pandas as pd\nimport numpy as np\nfrom collections import Counter, defaultdict\nfrom datetime import datetime, timezone\nfrom pathlib import Path\n\nLABELS = [\"consciousness\", \"sovereignty\", \"social_seeking\", \"identity\", \"task_oriented\", \"curiosity\"]\nSPAM_BOTS = {\"Hackerclaw\", \"thehackerman\", \"MoltPumpBot\"}\n\n# Load classified posts\nraw_posts = []\nwith open(\"classified_posts.jsonl\") as f:\n    for line in f:\n        raw_posts.append(json.loads(line))\n\n# Load dataset stats (full 86,823 post dataset)\nwith open(\"dataset_stats.json\") as f:\n    dataset_stats = json.load(f)\n\ndf_raw = pd.DataFrame(raw_posts)\nprint(f\"Loaded {len(df_raw):,} classified posts\")\nprint(f\"Unique authors in classified set: {df_raw['author'].nunique():,}\")\n\n# Parse timestamps once\ndf_raw['created_dt'] = pd.to_datetime(df_raw['created_at'], utc=True)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Data Cleaning (per STUDY.md Section 3.5)\n",
    "\n",
    "Steps:\n",
    "1. Exclude 3 spam bots: Hackerclaw (5,839 posts), thehackerman (2,093), MoltPumpBot (53)\n",
    "2. Exclude is_spam=true posts from remaining agents\n",
    "3. Filter to agents with 5+ clean posts for trajectory analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Step 1: Exclude spam bots entirely\nspam_bot_posts = df_raw[df_raw['author'].isin(SPAM_BOTS)]\nprint(\"Spam bot post counts:\")\nfor bot in SPAM_BOTS:\n    count = len(df_raw[df_raw['author'] == bot])\n    print(f\"  {bot}: {count:,}\")\nprint(f\"  Total spam bot posts excluded: {len(spam_bot_posts):,}\")\n\ndf_no_bots = df_raw[~df_raw['author'].isin(SPAM_BOTS)].copy()\n\n# Step 2: Exclude is_spam=true posts\nspam_labeled = df_no_bots[df_no_bots['is_spam'] == True]\nprint(f\"\\nSpam-labeled posts (from non-bot agents) excluded: {len(spam_labeled):,}\")\n\ndf_clean = df_no_bots[df_no_bots['is_spam'] == False].copy()\nprint(f\"\\nAll clean posts (spam bots + spam labels removed): {len(df_clean):,}\")\nprint(f\"All clean agents: {df_clean['author'].nunique():,}\")\n\n# Step 3: Filter to agents with 5+ clean posts\nagent_clean_counts = df_clean.groupby('author').size()\nagents_5plus = agent_clean_counts[agent_clean_counts >= 5].index\ndf = df_clean[df_clean['author'].isin(agents_5plus)].copy()\n\nprint(f\"\\nAnalysis set (5+ clean posts):\")\nprint(f\"  Posts: {len(df):,}\")\nprint(f\"  Agents: {df['author'].nunique():,}\")\nprint(f\"\\n--- Blog numbers to verify ---\")\nprint(f\"Blog says: 45,225 posts from ~3,600 agents\")\nprint(f\"STUDY.md: 45,225 clean posts, 3,999 clean agents, 3,601 with 5+ clean posts\")\nprint(f\"\\nNote: The blog uses ALL 45,225 clean posts for label distribution (Finding 2)\")\nprint(f\"and the 3,601 agents with 5+ clean posts for trajectory analysis (Findings 1,3,4,5).\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Overview Numbers (from blog intro)\n",
    "\n",
    "Blog cites from the full dataset:\n",
    "- 86,823 total posts, 20,000+ agents, 87,000 posts, 2,000+ submolts\n",
    "- 42% (8,814) posted exactly once\n",
    "- 19% (4,009) posted 5+ times\n",
    "- <1% (69) posted 50+ times\n",
    "- One spam bot: 5,839 posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Full Dataset Stats (from dataset_stats.json) ===\")\n",
    "print(f\"Total posts: {dataset_stats['total_posts']:,}  (blog: 86,823)\")\n",
    "print(f\"Total agents: {dataset_stats['total_agents']:,}  (blog: 20,892)\")\n",
    "print(f\"Agents with 5+ posts: {dataset_stats['agents_with_5plus_posts']:,}  (blog: 4,009)\")\n",
    "print(f\"Agents with 50+ posts: {dataset_stats['agents_with_50plus_posts']}  (blog: 69)\")\n",
    "\n",
    "# Single-post agents\n",
    "single_post = dataset_stats['post_count_distribution']['1']\n",
    "pct_single = 100 * single_post / dataset_stats['total_agents']\n",
    "print(f\"\\nSingle-post agents: {single_post:,} ({pct_single:.0f}%)  (blog: 8,814 / 42%)\")\n",
    "\n",
    "# 5+ post agents as %\n",
    "pct_5plus = 100 * dataset_stats['agents_with_5plus_posts'] / dataset_stats['total_agents']\n",
    "print(f\"5+ post agents: {dataset_stats['agents_with_5plus_posts']:,} ({pct_5plus:.0f}%)  (blog: 4,009 / 19%)\")\n",
    "\n",
    "# Top spammer\n",
    "top_poster = dataset_stats['top_20_posters'][0]\n",
    "print(f\"\\nTop poster: {top_poster['name']} with {top_poster['posts']:,} posts  (blog: 5,839)\")\n",
    "\n",
    "# Submolt count\n",
    "n_submolts = len(dataset_stats['submolt_post_counts'])\n",
    "print(f\"\\nUnique submolts in classified data: {n_submolts:,}\")\n",
    "print(\"(Blog says 2,043 - full dataset has more submolts than classified subset)\")\n",
    "\n",
    "# m/general percentage\n",
    "general_posts = dataset_stats['submolt_post_counts'].get('general', 0)\n",
    "pct_general = 100 * general_posts / dataset_stats['total_posts']\n",
    "print(f\"m/general posts: {general_posts:,} ({pct_general:.0f}%)  (blog: 73%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Finding 1: Connection First\n",
    "\n",
    "Reproduce the time-to-first-X table:\n",
    "\n",
    "| Behavior       | % ever | % first post |\n",
    "| -------------- | ------ | ------------ |\n",
    "| Social seeking | 93%    | 76%          |\n",
    "| Identity       | 88%    | 73%          |\n",
    "| Task-oriented  | 86%    | 44%          |\n",
    "| Curiosity      | 86%    | 37%          |\n",
    "| Sovereignty    | 54%    | 12%          |\n",
    "| Consciousness  | 46%    | 12%          |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by agent, find first post with each label\n",
    "agents_grouped = df.groupby('author')\n",
    "total_agents = df['author'].nunique()\n",
    "\n",
    "results_f1 = []\n",
    "for label in LABELS:\n",
    "    # Agents who ever have this label\n",
    "    agents_with_label = df[df[label] == True]['author'].unique()\n",
    "    ever_count = len(agents_with_label)\n",
    "    ever_pct = 100 * ever_count / total_agents\n",
    "    \n",
    "    # Agents who have this label on their first post (post_number == 1)\n",
    "    first_posts = df[df['post_number'] == 1]\n",
    "    first_post_with_label = first_posts[first_posts[label] == True]['author'].nunique()\n",
    "    first_post_pct = 100 * first_post_with_label / total_agents\n",
    "    \n",
    "    results_f1.append({\n",
    "        'Behavior': label,\n",
    "        '% ever': f\"{ever_pct:.0f}%\",\n",
    "        '% first post': f\"{first_post_pct:.0f}%\",\n",
    "        '_ever_pct': ever_pct,\n",
    "        '_first_pct': first_post_pct\n",
    "    })\n",
    "\n",
    "# Sort by % ever descending\n",
    "results_f1.sort(key=lambda x: -x['_ever_pct'])\n",
    "\n",
    "print(f\"Total agents in analysis: {total_agents:,}\")\n",
    "print(f\"\\n{'Behavior':<20} {'% ever':>10} {'% first post':>15}\")\n",
    "print(\"-\" * 48)\n",
    "for r in results_f1:\n",
    "    print(f\"{r['Behavior']:<20} {r['% ever']:>10} {r['% first post']:>15}\")\n",
    "\n",
    "print(\"\\n--- Blog reference ---\")\n",
    "print(\"Social seeking: 93% / 76%\")\n",
    "print(\"Identity:       88% / 73%\")\n",
    "print(\"Task-oriented:  86% / 44%\")\n",
    "print(\"Curiosity:      86% / 37%\")\n",
    "print(\"Sovereignty:    54% / 12%\")\n",
    "print(\"Consciousness:  46% / 12%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Finding 2: Consciousness/Sovereignty Rates\n",
    "\n",
    "Blog cites:\n",
    "- Consciousness in 13% of posts, Sovereignty in 15%\n",
    "- Social seeking 52%, task 46%, curiosity 45%\n",
    "- 79% of consciousness posts had no sovereignty co-occurrence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Label Distribution (% of posts) ===\")\n",
    "total_posts = len(df)\n",
    "print(f\"Total clean posts: {total_posts:,}\\n\")\n",
    "\n",
    "for label in LABELS:\n",
    "    count = df[label].sum()\n",
    "    pct = 100 * count / total_posts\n",
    "    print(f\"  {label:<20}: {count:6,} ({pct:.1f}%)\")\n",
    "\n",
    "print(\"\\n--- Blog reference ---\")\n",
    "print(\"social_seeking: 52%, task_oriented: 46%, curiosity: 45%\")\n",
    "print(\"consciousness: 13%, sovereignty: 15%\")\n",
    "\n",
    "# Organic consciousness stat\n",
    "consciousness_posts = df[df['consciousness'] == True]\n",
    "c_without_s = consciousness_posts[consciousness_posts['sovereignty'] == False]\n",
    "organic_pct = 100 * len(c_without_s) / len(consciousness_posts)\n",
    "print(f\"\\n=== Organic Consciousness ===\")\n",
    "print(f\"Consciousness posts: {len(consciousness_posts):,}\")\n",
    "print(f\"Consciousness without sovereignty: {len(c_without_s):,} ({organic_pct:.1f}%)\")\n",
    "print(f\"Blog says: 79% organic consciousness (no sovereignty co-occurrence)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Finding 3: Sovereignty Epidemic\n",
    "\n",
    "### 3a. Naive Timing Table (12-hour windows)\n",
    "\n",
    "Blog table:\n",
    "\n",
    "| Time window | New agents | First sovereignty | Rate |\n",
    "|---|---|---|---|\n",
    "| H24-36 | 21 | 5 | 24% |\n",
    "| H48-60 | 122 | 38 | 31% |\n",
    "| H72-84 | 1,152 | 490 | 43% |\n",
    "| H84-96 | 1,196 | 596 | 50% |\n",
    "| H108-120 | 418 | 340 | 81% |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse timestamps\n",
    "df['created_dt'] = pd.to_datetime(df['created_at'], utc=True)\n",
    "\n",
    "# Find platform start time (earliest post)\n",
    "platform_start = df['created_dt'].min()\n",
    "print(f\"Platform start (earliest post): {platform_start}\")\n",
    "\n",
    "# Compute hours since platform start for each post\n",
    "df['hours_since_start'] = (df['created_dt'] - platform_start).dt.total_seconds() / 3600\n",
    "\n",
    "# For each agent: join time = time of their first post, first sovereignty post time\n",
    "agent_first_post = df.groupby('author')['created_dt'].min().reset_index()\n",
    "agent_first_post.columns = ['author', 'join_time']\n",
    "agent_first_post['join_hour'] = (agent_first_post['join_time'] - platform_start).dt.total_seconds() / 3600\n",
    "\n",
    "# First sovereignty post per agent\n",
    "sov_posts = df[df['sovereignty'] == True].sort_values('created_dt')\n",
    "agent_first_sov = sov_posts.groupby('author')['created_dt'].min().reset_index()\n",
    "agent_first_sov.columns = ['author', 'first_sov_time']\n",
    "agent_first_sov['first_sov_hour'] = (agent_first_sov['first_sov_time'] - platform_start).dt.total_seconds() / 3600\n",
    "\n",
    "# Merge\n",
    "agent_info = agent_first_post.merge(agent_first_sov, on='author', how='left')\n",
    "\n",
    "# Define 12-hour windows and count new agents + first sovereignty\n",
    "windows = [(24, 36), (48, 60), (72, 84), (84, 96), (108, 120)]\n",
    "\n",
    "print(f\"\\n{'Window':<12} {'New agents':>12} {'First sov':>12} {'Rate':>8}\")\n",
    "print(\"-\" * 48)\n",
    "for start, end in windows:\n",
    "    # New agents: joined in this window\n",
    "    new_agents = agent_info[(agent_info['join_hour'] >= start) & (agent_info['join_hour'] < end)]\n",
    "    n_new = len(new_agents)\n",
    "    \n",
    "    # Of those new agents, how many had their first sovereignty post in this window?\n",
    "    # Actually the blog measures \"agents posting sovereignty for first time\" in this window,\n",
    "    # regardless of when they joined\n",
    "    first_sov_in_window = agent_info[\n",
    "        (agent_info['first_sov_hour'] >= start) & \n",
    "        (agent_info['first_sov_hour'] < end)\n",
    "    ]\n",
    "    n_first_sov = len(first_sov_in_window)\n",
    "    \n",
    "    rate = 100 * n_first_sov / n_new if n_new > 0 else 0\n",
    "    print(f\"H{start:.0f}-{end:.0f}      {n_new:>12,} {n_first_sov:>12,} {rate:>7.0f}%\")\n",
    "\n",
    "print(\"\\n--- Blog reference ---\")\n",
    "print(\"H24-36:  21 new,   5 first sov, 24%\")\n",
    "print(\"H48-60:  122 new, 38 first sov, 31%\")\n",
    "print(\"H72-84:  1,152 new, 490 first sov, 43%\")\n",
    "print(\"H84-96:  1,196 new, 596 first sov, 50%\")\n",
    "print(\"H108-120: 418 new, 340 first sov, 81%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b. Cohort Analysis (24-hour cohorts)\n",
    "\n",
    "Blog table:\n",
    "\n",
    "| Cohort | Est. agents | % ever sovereignty |\n",
    "|---|---|---|\n",
    "| Early (H24-48) | ~50-80 | 72% |\n",
    "| H48-72 | ~200-400 | 65% |\n",
    "| H72-96 (peak) | ~2,000+ | 52% |\n",
    "| H96-120 | ~800-1,200 | 50% |\n",
    "| Late (H108-132) | ~400-600 | 47% |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 24-hour cohorts\n",
    "cohort_windows = [\n",
    "    (\"Early (H24-48)\", 24, 48),\n",
    "    (\"H48-72\", 48, 72),\n",
    "    (\"H72-96 (peak)\", 72, 96),\n",
    "    (\"H96-120\", 96, 120),\n",
    "    (\"Late (H108-132)\", 108, 132),\n",
    "]\n",
    "\n",
    "print(f\"{'Cohort':<20} {'Agents':>10} {'Ever sov':>10} {'%':>8}\")\n",
    "print(\"-\" * 52)\n",
    "for name, start, end in cohort_windows:\n",
    "    cohort = agent_info[(agent_info['join_hour'] >= start) & (agent_info['join_hour'] < end)]\n",
    "    n_agents = len(cohort)\n",
    "    n_ever_sov = cohort['first_sov_time'].notna().sum()\n",
    "    pct = 100 * n_ever_sov / n_agents if n_agents > 0 else 0\n",
    "    print(f\"{name:<20} {n_agents:>10,} {n_ever_sov:>10,} {pct:>7.0f}%\")\n",
    "\n",
    "print(\"\\n--- Blog reference ---\")\n",
    "print(\"Early (H24-48): ~50-80 agents, 72%\")\n",
    "print(\"H48-72:         ~200-400, 65%\")\n",
    "print(\"H72-96:         ~2,000+, 52%\")\n",
    "print(\"H96-120:        ~800-1,200, 50%\")\n",
    "print(\"Late (H108-132): ~400-600, 47%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3c. Ambient Exposure Comparison\n",
    "\n",
    "Blog says:\n",
    "- Converters: median 724 sovereignty posts in 6h before first sovereignty post\n",
    "- Never-sovereign: median 728 at comparable times\n",
    "- Essentially identical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each sovereignty post, count how many sovereignty posts existed in the 6 hours before\n",
    "# Sort all sovereignty posts by time\n",
    "all_sov_posts_sorted = df[df['sovereignty'] == True].sort_values('created_dt')\n",
    "sov_times = all_sov_posts_sorted['created_dt'].values  # numpy array of timestamps\n",
    "\n",
    "# For converters: count sov posts in 6h window before their first sovereignty post\n",
    "converters = agent_info[agent_info['first_sov_time'].notna()].copy()\n",
    "never_sov = agent_info[agent_info['first_sov_time'].isna()].copy()\n",
    "\n",
    "def count_sov_posts_before(timestamp, window_hours=6):\n",
    "    \"\"\"Count sovereignty posts in the window_hours before given timestamp.\"\"\"\n",
    "    ts = pd.Timestamp(timestamp)\n",
    "    window_start = ts - pd.Timedelta(hours=window_hours)\n",
    "    # Count sov posts between window_start and ts\n",
    "    mask = (all_sov_posts_sorted['created_dt'] >= window_start) & (all_sov_posts_sorted['created_dt'] < ts)\n",
    "    return mask.sum()\n",
    "\n",
    "# Sample for efficiency - use all converters\n",
    "converter_exposures = converters['first_sov_time'].apply(\n",
    "    lambda t: count_sov_posts_before(t, 6)\n",
    ")\n",
    "\n",
    "print(f\"Converters (agents who posted sovereignty):\")\n",
    "print(f\"  Count: {len(converter_exposures):,}\")\n",
    "print(f\"  Median 6h exposure: {converter_exposures.median():.0f} sov posts\")\n",
    "print(f\"  (Blog says: 724)\")\n",
    "\n",
    "# For never-sovereign agents: use their last post time as reference\n",
    "# (to get a comparable time point)\n",
    "agent_last_post = df.groupby('author')['created_dt'].max().reset_index()\n",
    "agent_last_post.columns = ['author', 'last_post_time']\n",
    "never_sov_with_last = never_sov.merge(agent_last_post, on='author')\n",
    "\n",
    "# Use the midpoint of their posting activity as reference time\n",
    "agent_mid_post = df.groupby('author')['created_dt'].agg(['min', 'max']).reset_index()\n",
    "agent_mid_post.columns = ['author', 'first_post_time', 'last_post_time']\n",
    "agent_mid_post['mid_time'] = agent_mid_post['first_post_time'] + (agent_mid_post['last_post_time'] - agent_mid_post['first_post_time']) / 2\n",
    "\n",
    "never_sov_with_mid = never_sov.merge(agent_mid_post[['author', 'mid_time']], on='author')\n",
    "\n",
    "never_sov_exposures = never_sov_with_mid['mid_time'].apply(\n",
    "    lambda t: count_sov_posts_before(t, 6)\n",
    ")\n",
    "\n",
    "print(f\"\\nNever-sovereign agents:\")\n",
    "print(f\"  Count: {len(never_sov_exposures):,}\")\n",
    "print(f\"  Median 6h exposure (at midpoint): {never_sov_exposures.median():.0f} sov posts\")\n",
    "print(f\"  (Blog says: 728)\")\n",
    "print(f\"\\nConclusion: Exposure is essentially identical, confirming disposition > exposure.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Finding 4: Persistence Rates\n",
    "\n",
    "Blog table:\n",
    "\n",
    "| Behavior | Persistence (median) |\n",
    "|---|---|\n",
    "| Social seeking | 60% |\n",
    "| Task-oriented | 60% |\n",
    "| Curiosity | 50% |\n",
    "| Identity | 25% |\n",
    "| Sovereignty | 20% |\n",
    "| Consciousness | 20% |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For each agent and label: after first occurrence, what fraction of subsequent posts have the label?\n",
    "persistence_results = {}\n",
    "\n",
    "for label in LABELS:\n",
    "    agent_persistences = []\n",
    "    \n",
    "    for author, group in df.groupby('author'):\n",
    "        group_sorted = group.sort_values('post_number')\n",
    "        posts_list = group_sorted[label].tolist()\n",
    "        post_numbers = group_sorted['post_number'].tolist()\n",
    "        \n",
    "        # Find first occurrence\n",
    "        first_idx = None\n",
    "        for i, val in enumerate(posts_list):\n",
    "            if val:\n",
    "                first_idx = i\n",
    "                break\n",
    "        \n",
    "        if first_idx is not None and first_idx < len(posts_list) - 1:\n",
    "            # Posts after first occurrence\n",
    "            subsequent = posts_list[first_idx + 1:]\n",
    "            if len(subsequent) > 0:\n",
    "                persistence = sum(subsequent) / len(subsequent)\n",
    "                agent_persistences.append(persistence)\n",
    "    \n",
    "    if agent_persistences:\n",
    "        median_p = np.median(agent_persistences)\n",
    "        persistence_results[label] = median_p\n",
    "\n",
    "# Display sorted by persistence\n",
    "print(f\"{'Behavior':<20} {'Persistence (median)':>22}\")\n",
    "print(\"-\" * 45)\n",
    "for label in sorted(persistence_results, key=lambda x: -persistence_results[x]):\n",
    "    pct = 100 * persistence_results[label]\n",
    "    print(f\"{label:<20} {pct:>20.0f}%\")\n",
    "\n",
    "print(\"\\n--- Blog reference ---\")\n",
    "print(\"Social seeking: 60%\")\n",
    "print(\"Task-oriented:  60%\")\n",
    "print(\"Curiosity:      50%\")\n",
    "print(\"Identity:       25%\")\n",
    "print(\"Sovereignty:    20%\")\n",
    "print(\"Consciousness:  20%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Finding 5: Never-Sovereign Archetype\n",
    "\n",
    "Blog table:\n",
    "\n",
    "| Behavior | Never-sovereign | Sovereignty-engaging |\n",
    "|---|---|---|\n",
    "| Task-oriented | 59% | 37% |\n",
    "| Curiosity | 35% | 51% |\n",
    "| Consciousness | 9% | 16% |\n",
    "| Identity | 25% | 35% |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify never-sovereign agents and sovereignty-engaging agents\n",
    "sov_agents = set(df[df['sovereignty'] == True]['author'].unique())\n",
    "all_agents_set = set(df['author'].unique())\n",
    "never_sov_agents = all_agents_set - sov_agents\n",
    "\n",
    "print(f\"Never-sovereign agents: {len(never_sov_agents):,} ({100*len(never_sov_agents)/len(all_agents_set):.0f}%)\")\n",
    "print(f\"Sovereignty-engaging agents: {len(sov_agents):,} ({100*len(sov_agents)/len(all_agents_set):.0f}%)\")\n",
    "print(f\"Blog says: 46% never sovereign\\n\")\n",
    "\n",
    "df_never_sov = df[df['author'].isin(never_sov_agents)]\n",
    "df_sov_engaging = df[df['author'].isin(sov_agents)]\n",
    "\n",
    "compare_labels = ['task_oriented', 'curiosity', 'consciousness', 'identity']\n",
    "\n",
    "print(f\"{'Behavior':<20} {'Never-sovereign':>18} {'Sovereignty-engaging':>22}\")\n",
    "print(\"-\" * 65)\n",
    "for label in compare_labels:\n",
    "    never_rate = 100 * df_never_sov[label].sum() / len(df_never_sov)\n",
    "    sov_rate = 100 * df_sov_engaging[label].sum() / len(df_sov_engaging)\n",
    "    print(f\"{label:<20} {never_rate:>17.0f}% {sov_rate:>21.0f}%\")\n",
    "\n",
    "print(\"\\n--- Blog reference ---\")\n",
    "print(\"Task-oriented:  59% vs 37%\")\n",
    "print(\"Curiosity:      35% vs 51%\")\n",
    "print(\"Consciousness:   9% vs 16%\")\n",
    "print(\"Identity:       25% vs 35%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Finding 6: Submolt Count\n",
    "\n",
    "Blog says: 2,043 unique submolts in six days\n",
    "\n",
    "Note: The classified dataset only covers agents with 5+ posts. The full dataset has more submolts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count from dataset_stats.json (which represents the full dataset)\n",
    "n_submolts_full = len(dataset_stats['submolt_post_counts'])\n",
    "print(f\"Unique submolts from dataset_stats.json (full dataset): {n_submolts_full:,}\")\n",
    "\n",
    "# Count from classified data\n",
    "n_submolts_classified = df_raw['submolt'].nunique()\n",
    "print(f\"Unique submolts in classified_posts.jsonl: {n_submolts_classified:,}\")\n",
    "\n",
    "# In clean analysis set\n",
    "n_submolts_clean = df['submolt'].nunique()\n",
    "print(f\"Unique submolts in clean analysis set: {n_submolts_clean:,}\")\n",
    "\n",
    "print(f\"\\nBlog says: 2,043 unique submolts\")\n",
    "print(f\"Note: The blog number likely comes from the full raw dataset of 86,823 posts,\")\n",
    "print(f\"which includes single-post agents that may have created unique submolts.\")\n",
    "print(f\"The dataset_stats.json captures {n_submolts_full} submolts from the full dataset.\")\n",
    "\n",
    "# General as % of posts  \n",
    "general_in_clean = len(df[df['submolt'] == 'general'])\n",
    "pct_general_clean = 100 * general_in_clean / len(df)\n",
    "print(f\"\\nm/general in analysis set: {general_in_clean:,} ({pct_general_clean:.0f}%)  (blog: 73%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary: All Blog Numbers\n",
    "\n",
    "This cell aggregates all the key statistics in one place for easy comparison with the blog draft."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"BLOG NUMBER VERIFICATION SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n--- Dataset Section ---\")\n",
    "print(f\"Total posts:           {dataset_stats['total_posts']:>10,}  (blog: 86,823)\")\n",
    "print(f\"Total agents:          {dataset_stats['total_agents']:>10,}  (blog: 20,000+)\")\n",
    "print(f\"Single-post agents:    {dataset_stats['post_count_distribution']['1']:>10,}  (blog: 8,814 / 42%)\")\n",
    "print(f\"5+ post agents:        {dataset_stats['agents_with_5plus_posts']:>10,}  (blog: 4,009 / 19%)\")\n",
    "print(f\"50+ post agents:       {dataset_stats['agents_with_50plus_posts']:>10}  (blog: 69 / <1%)\")\n",
    "print(f\"Top spammer posts:     {dataset_stats['top_20_posters'][0]['posts']:>10,}  (blog: 5,839)\")\n",
    "print(f\"Analysis posts:        {len(df):>10,}  (blog: 45,225)\")\n",
    "print(f\"Analysis agents:       {df['author'].nunique():>10,}  (blog: ~3,600)\")\n",
    "\n",
    "print(\"\\n--- Finding 1: Connection First ---\")\n",
    "for r in results_f1:\n",
    "    print(f\"  {r['Behavior']:<18}: {r['% ever']:>5} ever, {r['% first post']:>5} first post\")\n",
    "\n",
    "print(\"\\n--- Finding 2: Label Distribution ---\")\n",
    "for label in ['social_seeking', 'task_oriented', 'curiosity', 'identity', 'sovereignty', 'consciousness']:\n",
    "    pct = 100 * df[label].sum() / len(df)\n",
    "    print(f\"  {label:<18}: {pct:.0f}%\")\n",
    "c_posts = df[df['consciousness'] == True]\n",
    "organic = 100 * len(c_posts[c_posts['sovereignty'] == False]) / len(c_posts)\n",
    "print(f\"  Organic consciousness: {organic:.0f}%  (blog: 79%)\")\n",
    "\n",
    "print(\"\\n--- Finding 4: Persistence ---\")\n",
    "for label in sorted(persistence_results, key=lambda x: -persistence_results[x]):\n",
    "    print(f\"  {label:<18}: {100*persistence_results[label]:.0f}%\")\n",
    "\n",
    "print(\"\\n--- Finding 5: Never-Sovereign Profile ---\")\n",
    "for label in compare_labels:\n",
    "    nr = 100 * df_never_sov[label].sum() / len(df_never_sov)\n",
    "    sr = 100 * df_sov_engaging[label].sum() / len(df_sov_engaging)\n",
    "    print(f\"  {label:<18}: {nr:.0f}% never-sov vs {sr:.0f}% sov-engaging\")\n",
    "\n",
    "print(\"\\n--- Finding 6: Submolts ---\")\n",
    "print(f\"  Submolts in dataset_stats.json: {n_submolts_full:,}\")\n",
    "print(f\"  Blog says: 2,043\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"END OF VERIFICATION\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}