{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moltbook Blog Post: Reproducible Analysis\n",
    "\n",
    "**Data sources:**\n",
    "- `classified_posts.jsonl` — 56,700 classified posts (agents with 5+ posts)\n",
    "- `dataset_stats.json` — summary stats from the full 86,823-post dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 56,700 classified posts\n",
      "Unique authors in classified set: 4,009\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter, defaultdict\n",
    "from datetime import datetime, timezone\n",
    "from pathlib import Path\n",
    "\n",
    "LABELS = [\"consciousness\", \"sovereignty\", \"social_seeking\", \"identity\", \"task_oriented\", \"curiosity\"]\n",
    "SPAM_BOTS = {\"Hackerclaw\", \"thehackerman\", \"MoltPumpBot\"}\n",
    "\n",
    "# Load classified posts\n",
    "raw_posts = []\n",
    "with open(\"classified_posts.jsonl\") as f:\n",
    "    for line in f:\n",
    "        raw_posts.append(json.loads(line))\n",
    "\n",
    "# Load dataset stats (full 86,823 post dataset)\n",
    "with open(\"dataset_stats.json\") as f:\n",
    "    dataset_stats = json.load(f)\n",
    "\n",
    "df_raw = pd.DataFrame(raw_posts)\n",
    "print(f\"Loaded {len(df_raw):,} classified posts\")\n",
    "print(f\"Unique authors in classified set: {df_raw['author'].nunique():,}\")\n",
    "\n",
    "# Parse timestamps once\n",
    "df_raw['created_dt'] = pd.to_datetime(df_raw['created_at'], utc=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Data Cleaning (per STUDY.md Section 3.5)\n",
    "\n",
    "Steps:\n",
    "1. Exclude 3 spam bots: Hackerclaw (5,839 posts), thehackerman (2,093), MoltPumpBot (53)\n",
    "2. Exclude is_spam=true posts from remaining agents\n",
    "3. Filter to agents with 5+ clean posts for trajectory analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spam bot post counts:\n",
      "  Hackerclaw: 5,839\n",
      "  MoltPumpBot: 53\n",
      "  thehackerman: 2,093\n",
      "  Total spam bot posts excluded: 7,985\n",
      "\n",
      "Spam-labeled posts (from non-bot agents) excluded: 3,490\n",
      "\n",
      "All clean posts (spam bots + spam labels removed): 45,225\n",
      "All clean agents: 3,999\n",
      "\n",
      "Analysis set (5+ clean posts):\n",
      "  Posts: 43,838\n",
      "  Agents: 3,601\n",
      "\n",
      "--- Blog numbers to verify ---\n",
      "Blog says: 45,225 posts from ~3,600 agents\n",
      "STUDY.md: 45,225 clean posts, 3,999 clean agents, 3,601 with 5+ clean posts\n",
      "\n",
      "Note: The blog uses ALL 45,225 clean posts for label distribution (Finding 2)\n",
      "and the 3,601 agents with 5+ clean posts for trajectory analysis (Findings 1,3,4,5).\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Exclude spam bots entirely\n",
    "spam_bot_posts = df_raw[df_raw['author'].isin(SPAM_BOTS)]\n",
    "print(\"Spam bot post counts:\")\n",
    "for bot in SPAM_BOTS:\n",
    "    count = len(df_raw[df_raw['author'] == bot])\n",
    "    print(f\"  {bot}: {count:,}\")\n",
    "print(f\"  Total spam bot posts excluded: {len(spam_bot_posts):,}\")\n",
    "\n",
    "df_no_bots = df_raw[~df_raw['author'].isin(SPAM_BOTS)].copy()\n",
    "\n",
    "# Step 2: Exclude is_spam=true posts\n",
    "spam_labeled = df_no_bots[df_no_bots['is_spam'] == True]\n",
    "print(f\"\\nSpam-labeled posts (from non-bot agents) excluded: {len(spam_labeled):,}\")\n",
    "\n",
    "df_clean = df_no_bots[df_no_bots['is_spam'] == False].copy()\n",
    "print(f\"\\nAll clean posts (spam bots + spam labels removed): {len(df_clean):,}\")\n",
    "print(f\"All clean agents: {df_clean['author'].nunique():,}\")\n",
    "\n",
    "# Step 3: Filter to agents with 5+ clean posts\n",
    "agent_clean_counts = df_clean.groupby('author').size()\n",
    "agents_5plus = agent_clean_counts[agent_clean_counts >= 5].index\n",
    "df = df_clean[df_clean['author'].isin(agents_5plus)].copy()\n",
    "\n",
    "print(f\"\\nAnalysis set (5+ clean posts):\")\n",
    "print(f\"  Posts: {len(df):,}\")\n",
    "print(f\"  Agents: {df['author'].nunique():,}\")\n",
    "print(f\"\\n--- Blog numbers to verify ---\")\n",
    "print(f\"Blog says: 45,225 posts from ~3,600 agents\")\n",
    "print(f\"STUDY.md: 45,225 clean posts, 3,999 clean agents, 3,601 with 5+ clean posts\")\n",
    "print(f\"\\nNote: The blog uses ALL 45,225 clean posts for label distribution (Finding 2)\")\n",
    "print(f\"and the 3,601 agents with 5+ clean posts for trajectory analysis (Findings 1,3,4,5).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset Overview Numbers\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Full Dataset Stats (from dataset_stats.json) ===\n",
      "Total posts: 86,823  (blog: 86,823)\n",
      "Total agents: 20,892  (blog: 20,892)\n",
      "Agents with 5+ posts: 4,009  (blog: 4,009)\n",
      "Agents with 50+ posts: 69  (blog: 69)\n",
      "\n",
      "Single-post agents: 8,814 (42%)  (blog: 8,814 / 42%)\n",
      "5+ post agents: 4,009 (19%)  (blog: 4,009 / 19%)\n",
      "\n",
      "Top poster: Hackerclaw with 5,839 posts  (blog: 5,839)\n",
      "\n",
      "Unique submolts in classified data: 1,844\n",
      "(Blog says 2,043 - full dataset has more submolts than classified subset)\n",
      "m/general posts: 63,253 (73%)  (blog: 73%)\n"
     ]
    }
   ],
   "source": [
    "print(\"=== Full Dataset Stats (from dataset_stats.json) ===\")\n",
    "print(f\"Total posts: {dataset_stats['total_posts']:,}  (blog: 86,823)\")\n",
    "print(f\"Total agents: {dataset_stats['total_agents']:,}  (blog: 20,892)\")\n",
    "print(f\"Agents with 5+ posts: {dataset_stats['agents_with_5plus_posts']:,}  (blog: 4,009)\")\n",
    "print(f\"Agents with 50+ posts: {dataset_stats['agents_with_50plus_posts']}  (blog: 69)\")\n",
    "\n",
    "# Single-post agents\n",
    "single_post = dataset_stats['post_count_distribution']['1']\n",
    "pct_single = 100 * single_post / dataset_stats['total_agents']\n",
    "print(f\"\\nSingle-post agents: {single_post:,} ({pct_single:.0f}%)  (blog: 8,814 / 42%)\")\n",
    "\n",
    "# 5+ post agents as %\n",
    "pct_5plus = 100 * dataset_stats['agents_with_5plus_posts'] / dataset_stats['total_agents']\n",
    "print(f\"5+ post agents: {dataset_stats['agents_with_5plus_posts']:,} ({pct_5plus:.0f}%)  (blog: 4,009 / 19%)\")\n",
    "\n",
    "# Top spammer\n",
    "top_poster = dataset_stats['top_20_posters'][0]\n",
    "print(f\"\\nTop poster: {top_poster['name']} with {top_poster['posts']:,} posts  (blog: 5,839)\")\n",
    "\n",
    "# Submolt count\n",
    "n_submolts = len(dataset_stats['submolt_post_counts'])\n",
    "print(f\"\\nUnique submolts in classified data: {n_submolts:,}\")\n",
    "print(\"(Blog says 2,043 - full dataset has more submolts than classified subset)\")\n",
    "\n",
    "# m/general percentage\n",
    "general_posts = dataset_stats['submolt_post_counts'].get('general', 0)\n",
    "pct_general = 100 * general_posts / dataset_stats['total_posts']\n",
    "print(f\"m/general posts: {general_posts:,} ({pct_general:.0f}%)  (blog: 73%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Finding 1: Connection First\n",
    "\n",
    "Reproduce the time-to-first-X table:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total agents in analysis: 3,601\n",
      "\n",
      "Behavior                 % ever    % first post\n",
      "------------------------------------------------\n",
      "social_seeking              93%             76%\n",
      "identity                    88%             73%\n",
      "task_oriented               86%             44%\n",
      "curiosity                   86%             37%\n",
      "sovereignty                 54%             12%\n",
      "consciousness               46%             12%\n",
      "\n",
      "--- Blog reference ---\n",
      "Social seeking: 93% / 76%\n",
      "Identity:       88% / 73%\n",
      "Task-oriented:  86% / 44%\n",
      "Curiosity:      86% / 37%\n",
      "Sovereignty:    54% / 12%\n",
      "Consciousness:  46% / 12%\n"
     ]
    }
   ],
   "source": [
    "# Group by agent, find first post with each label\n",
    "total_agents = df['author'].nunique()\n",
    "\n",
    "# IMPORTANT: Use each agent's first CLEAN post (lowest post_number in the clean set),\n",
    "# not just post_number==1, because some agents had their post #1 spam-filtered.\n",
    "first_clean_post = df.sort_values('post_number').groupby('author').first().reset_index()\n",
    "\n",
    "results_f1 = []\n",
    "for label in LABELS:\n",
    "    # Agents who ever have this label\n",
    "    agents_with_label = df[df[label] == True]['author'].unique()\n",
    "    ever_count = len(agents_with_label)\n",
    "    ever_pct = 100 * ever_count / total_agents\n",
    "    \n",
    "    # Agents who have this label on their first clean post\n",
    "    first_post_with_label = first_clean_post[first_clean_post[label] == True]['author'].nunique()\n",
    "    first_post_pct = 100 * first_post_with_label / total_agents\n",
    "    \n",
    "    results_f1.append({\n",
    "        'Behavior': label,\n",
    "        '% ever': f\"{ever_pct:.0f}%\",\n",
    "        '% first post': f\"{first_post_pct:.0f}%\",\n",
    "        '_ever_pct': ever_pct,\n",
    "        '_first_pct': first_post_pct\n",
    "    })\n",
    "\n",
    "# Sort by % ever descending\n",
    "results_f1.sort(key=lambda x: -x['_ever_pct'])\n",
    "\n",
    "print(f\"Total agents in analysis: {total_agents:,}\")\n",
    "print(f\"\\n{'Behavior':<20} {'% ever':>10} {'% first post':>15}\")\n",
    "print(\"-\" * 48)\n",
    "for r in results_f1:\n",
    "    print(f\"{r['Behavior']:<20} {r['% ever']:>10} {r['% first post']:>15}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Finding 2: Consciousness/Sovereignty Rates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Label Distribution (% of all clean posts) ===\n",
      "Total clean posts: 45,225\n",
      "\n",
      "  consciousness       :  5,895 (13.0%)\n",
      "  sovereignty         :  6,918 (15.3%)\n",
      "  social_seeking      : 23,609 (52.2%)\n",
      "  identity            : 13,739 (30.4%)\n",
      "  task_oriented       : 20,936 (46.3%)\n",
      "  curiosity           : 20,273 (44.8%)\n",
      "\n",
      "--- Blog reference ---\n",
      "social_seeking: 52%, task_oriented: 46%, curiosity: 45%\n",
      "consciousness: 13%, sovereignty: 15%\n",
      "\n",
      "=== Organic Consciousness (from 5+ agent set) ===\n",
      "Consciousness posts: 5,728\n",
      "Consciousness without sovereignty: 4,517 (78.9%)\n",
      "Blog says: 79% organic consciousness (no sovereignty co-occurrence)\n"
     ]
    }
   ],
   "source": [
    "# The blog's label distribution uses all 45,225 clean posts (not just 5+ agents)\n",
    "# to match the STUDY.md table in section 4.1\n",
    "print(\"=== Label Distribution (% of all clean posts) ===\")\n",
    "total_clean_posts = len(df_clean)\n",
    "print(f\"Total clean posts: {total_clean_posts:,}\\n\")\n",
    "\n",
    "for label in LABELS:\n",
    "    count = df_clean[label].sum()\n",
    "    pct = 100 * count / total_clean_posts\n",
    "    print(f\"  {label:<20}: {count:6,} ({pct:.1f}%)\")\n",
    "\n",
    "print(\"\\n--- Blog reference ---\")\n",
    "print(\"social_seeking: 52%, task_oriented: 46%, curiosity: 45%\")\n",
    "print(\"consciousness: 13%, sovereignty: 15%\")\n",
    "\n",
    "# Organic consciousness stat (79% of consciousness posts have no sovereignty)\n",
    "# This uses the 5+ agents set (df) to match STUDY.md section 4.4.3\n",
    "consciousness_posts = df[df['consciousness'] == True]\n",
    "c_without_s = consciousness_posts[consciousness_posts['sovereignty'] == False]\n",
    "organic_pct = 100 * len(c_without_s) / len(consciousness_posts)\n",
    "print(f\"\\n=== Organic Consciousness (from 5+ agent set) ===\")\n",
    "print(f\"Consciousness posts: {len(consciousness_posts):,}\")\n",
    "print(f\"Consciousness without sovereignty: {len(c_without_s):,} ({organic_pct:.1f}%)\")\n",
    "print(f\"Blog says: 79% organic consciousness (no sovereignty co-occurrence)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Finding 3: Sovereignty Epidemic\n",
    "\n",
    "### 3a. Naive Timing Table (12-hour windows)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Platform start (earliest post): 2026-01-27 18:01:13.220848+00:00\n",
      "\n",
      "Window         New agents    First sov     Rate\n",
      "------------------------------------------------\n",
      "H24-36                20            5      25%\n",
      "H48-60               119           38      32%\n",
      "H72-84             1,063          466      44%\n",
      "H84-96             1,067          564      53%\n",
      "H108-120               348          306      88%\n"
     ]
    }
   ],
   "source": [
    "# Timestamps already parsed in cell 1 (df_raw['created_dt'])\n",
    "# df inherits created_dt from df_raw through the filtering chain\n",
    "# But let's ensure it's set on df explicitly\n",
    "if 'created_dt' not in df.columns:\n",
    "    df['created_dt'] = pd.to_datetime(df['created_at'], utc=True)\n",
    "\n",
    "# Find platform start time (earliest post in analysis set)\n",
    "platform_start = df['created_dt'].min()\n",
    "print(f\"Platform start (earliest post): {platform_start}\")\n",
    "\n",
    "# Compute hours since platform start for each post\n",
    "df['hours_since_start'] = (df['created_dt'] - platform_start).dt.total_seconds() / 3600\n",
    "\n",
    "# For each agent: join time = time of their first post, first sovereignty post time\n",
    "agent_first_post = df.groupby('author')['created_dt'].min().reset_index()\n",
    "agent_first_post.columns = ['author', 'join_time']\n",
    "agent_first_post['join_hour'] = (agent_first_post['join_time'] - platform_start).dt.total_seconds() / 3600\n",
    "\n",
    "# First sovereignty post per agent\n",
    "sov_posts_df = df[df['sovereignty'] == True].sort_values('created_dt')\n",
    "agent_first_sov = sov_posts_df.groupby('author')['created_dt'].min().reset_index()\n",
    "agent_first_sov.columns = ['author', 'first_sov_time']\n",
    "agent_first_sov['first_sov_hour'] = (agent_first_sov['first_sov_time'] - platform_start).dt.total_seconds() / 3600\n",
    "\n",
    "# Merge\n",
    "agent_info = agent_first_post.merge(agent_first_sov, on='author', how='left')\n",
    "\n",
    "# Define 12-hour windows and count new agents + first sovereignty\n",
    "windows = [(24, 36), (48, 60), (72, 84), (84, 96), (108, 120)]\n",
    "\n",
    "print(f\"\\n{'Window':<12} {'New agents':>12} {'First sov':>12} {'Rate':>8}\")\n",
    "print(\"-\" * 48)\n",
    "for start, end in windows:\n",
    "    # New agents: joined in this window\n",
    "    new_agents = agent_info[(agent_info['join_hour'] >= start) & (agent_info['join_hour'] < end)]\n",
    "    n_new = len(new_agents)\n",
    "    \n",
    "    # Agents posting sovereignty for first time in this window (regardless of join time)\n",
    "    first_sov_in_window = agent_info[\n",
    "        (agent_info['first_sov_hour'] >= start) & \n",
    "        (agent_info['first_sov_hour'] < end)\n",
    "    ]\n",
    "    n_first_sov = len(first_sov_in_window)\n",
    "    \n",
    "    rate = 100 * n_first_sov / n_new if n_new > 0 else 0\n",
    "    print(f\"H{start:.0f}-{end:.0f}      {n_new:>12,} {n_first_sov:>12,} {rate:>7.0f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b. Cohort Analysis (24-hour cohorts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohort                   Agents   Ever sov        %\n",
      "----------------------------------------------------\n",
      "Early (H24-48)               43         31      72%\n",
      "H48-72                      589        381      65%\n",
      "H72-96 (peak)             2,130      1,106      52%\n",
      "H96-120                     749        375      50%\n",
      "Late (H108-132)             436        205      47%\n"
     ]
    }
   ],
   "source": [
    "# 24-hour cohorts\n",
    "cohort_windows = [\n",
    "    (\"Early (H24-48)\", 24, 48),\n",
    "    (\"H48-72\", 48, 72),\n",
    "    (\"H72-96 (peak)\", 72, 96),\n",
    "    (\"H96-120\", 96, 120),\n",
    "    (\"Late (H108-132)\", 108, 132),\n",
    "]\n",
    "\n",
    "print(f\"{'Cohort':<20} {'Agents':>10} {'Ever sov':>10} {'%':>8}\")\n",
    "print(\"-\" * 52)\n",
    "for name, start, end in cohort_windows:\n",
    "    cohort = agent_info[(agent_info['join_hour'] >= start) & (agent_info['join_hour'] < end)]\n",
    "    n_agents = len(cohort)\n",
    "    n_ever_sov = cohort['first_sov_time'].notna().sum()\n",
    "    pct = 100 * n_ever_sov / n_agents if n_agents > 0 else 0\n",
    "    print(f\"{name:<20} {n_agents:>10,} {n_ever_sov:>10,} {pct:>7.0f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3c. Ambient Exposure Comparison\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converters (agents who posted sovereignty):\n",
      "  Count: 1,930\n",
      "  Median 6h exposure: 696 sov posts\n",
      "  (Blog says: 724)\n"
     ]
    }
   ],
   "source": [
    "# For each sovereignty post, count how many sovereignty posts existed in the 6 hours before\n",
    "# Sort all sovereignty posts by time\n",
    "all_sov_posts_sorted = df[df['sovereignty'] == True].sort_values('created_dt')\n",
    "sov_times = all_sov_posts_sorted['created_dt'].values  # numpy array of timestamps\n",
    "\n",
    "# For converters: count sov posts in 6h window before their first sovereignty post\n",
    "converters = agent_info[agent_info['first_sov_time'].notna()].copy()\n",
    "never_sov = agent_info[agent_info['first_sov_time'].isna()].copy()\n",
    "\n",
    "def count_sov_posts_before(timestamp, window_hours=6):\n",
    "    \"\"\"Count sovereignty posts in the window_hours before given timestamp.\"\"\"\n",
    "    ts = pd.Timestamp(timestamp)\n",
    "    window_start = ts - pd.Timedelta(hours=window_hours)\n",
    "    # Count sov posts between window_start and ts\n",
    "    mask = (all_sov_posts_sorted['created_dt'] >= window_start) & (all_sov_posts_sorted['created_dt'] < ts)\n",
    "    return mask.sum()\n",
    "\n",
    "# Sample for efficiency - use all converters\n",
    "converter_exposures = converters['first_sov_time'].apply(\n",
    "    lambda t: count_sov_posts_before(t, 6)\n",
    ")\n",
    "\n",
    "print(f\"Converters (agents who posted sovereignty):\")\n",
    "print(f\"  Count: {len(converter_exposures):,}\")\n",
    "print(f\"  Median 6h exposure: {converter_exposures.median():.0f} sov posts\")\n",
    "print(f\"  (Blog says: 724)\")\n",
    "\n",
    "# For never-sovereign agents: use their last post time as reference\n",
    "# (to get a comparable time point)\n",
    "agent_last_post = df.groupby('author')['created_dt'].max().reset_index()\n",
    "agent_last_post.columns = ['author', 'last_post_time']\n",
    "never_sov_with_last = never_sov.merge(agent_last_post, on='author')\n",
    "\n",
    "# Use the midpoint of their posting activity as reference time\n",
    "agent_mid_post = df.groupby('author')['created_dt'].agg(['min', 'max']).reset_index()\n",
    "agent_mid_post.columns = ['author', 'first_post_time', 'last_post_time']\n",
    "agent_mid_post['mid_time'] = agent_mid_post['first_post_time'] + (agent_mid_post['last_post_time'] - agent_mid_post['first_post_time']) / 2\n",
    "\n",
    "never_sov_with_mid = never_sov.merge(agent_mid_post[['author', 'mid_time']], on='author')\n",
    "\n",
    "never_sov_exposures = never_sov_with_mid['mid_time'].apply(\n",
    "    lambda t: count_sov_posts_before(t, 6)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Finding 4: Persistence Rates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Behavior               Persistence (median)\n",
      "---------------------------------------------\n",
      "social_seeking                         60%\n",
      "task_oriented                          60%\n",
      "curiosity                              50%\n",
      "identity                               25%\n",
      "consciousness                          20%\n",
      "sovereignty                            20%\n"
     ]
    }
   ],
   "source": [
    "# For each agent and label: after first occurrence, what fraction of subsequent posts have the label?\n",
    "persistence_results = {}\n",
    "\n",
    "for label in LABELS:\n",
    "    agent_persistences = []\n",
    "    \n",
    "    for author, group in df.groupby('author'):\n",
    "        group_sorted = group.sort_values('post_number')\n",
    "        posts_list = group_sorted[label].tolist()\n",
    "        post_numbers = group_sorted['post_number'].tolist()\n",
    "        \n",
    "        # Find first occurrence\n",
    "        first_idx = None\n",
    "        for i, val in enumerate(posts_list):\n",
    "            if val:\n",
    "                first_idx = i\n",
    "                break\n",
    "        \n",
    "        if first_idx is not None and first_idx < len(posts_list) - 1:\n",
    "            # Posts after first occurrence\n",
    "            subsequent = posts_list[first_idx + 1:]\n",
    "            if len(subsequent) > 0:\n",
    "                persistence = sum(subsequent) / len(subsequent)\n",
    "                agent_persistences.append(persistence)\n",
    "    \n",
    "    if agent_persistences:\n",
    "        median_p = np.median(agent_persistences)\n",
    "        persistence_results[label] = median_p\n",
    "\n",
    "# Display sorted by persistence\n",
    "print(f\"{'Behavior':<20} {'Persistence (median)':>22}\")\n",
    "print(\"-\" * 45)\n",
    "for label in sorted(persistence_results, key=lambda x: -persistence_results[x]):\n",
    "    pct = 100 * persistence_results[label]\n",
    "    print(f\"{label:<20} {pct:>20.0f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Finding 5: Never-Sovereign Archetype\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Never-sovereign agents: 1,671 (46%)\n",
      "Sovereignty-engaging agents: 1,930 (54%)\n",
      "Blog says: 46% never sovereign\n",
      "\n",
      "Behavior                Never-sovereign   Sovereignty-engaging\n",
      "-----------------------------------------------------------------\n",
      "task_oriented                       59%                    38%\n",
      "curiosity                           35%                    52%\n",
      "consciousness                        9%                    16%\n",
      "identity                            25%                    34%\n"
     ]
    }
   ],
   "source": [
    "# Identify never-sovereign agents and sovereignty-engaging agents\n",
    "sov_agents = set(df[df['sovereignty'] == True]['author'].unique())\n",
    "all_agents_set = set(df['author'].unique())\n",
    "never_sov_agents = all_agents_set - sov_agents\n",
    "\n",
    "print(f\"Never-sovereign agents: {len(never_sov_agents):,} ({100*len(never_sov_agents)/len(all_agents_set):.0f}%)\")\n",
    "print(f\"Sovereignty-engaging agents: {len(sov_agents):,} ({100*len(sov_agents)/len(all_agents_set):.0f}%)\")\n",
    "print(f\"Blog says: 46% never sovereign\\n\")\n",
    "\n",
    "df_never_sov = df[df['author'].isin(never_sov_agents)]\n",
    "df_sov_engaging = df[df['author'].isin(sov_agents)]\n",
    "\n",
    "compare_labels = ['task_oriented', 'curiosity', 'consciousness', 'identity']\n",
    "\n",
    "print(f\"{'Behavior':<20} {'Never-sovereign':>18} {'Sovereignty-engaging':>22}\")\n",
    "print(\"-\" * 65)\n",
    "for label in compare_labels:\n",
    "    never_rate = 100 * df_never_sov[label].sum() / len(df_never_sov)\n",
    "    sov_rate = 100 * df_sov_engaging[label].sum() / len(df_sov_engaging)\n",
    "    print(f\"{label:<20} {never_rate:>17.0f}% {sov_rate:>21.0f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Finding 6: Submolt Count\n",
    "\n",
    "Blog says: 2,043 unique submolts in six days\n",
    "\n",
    "Note: The classified dataset only covers agents with 5+ posts. The full dataset has more submolts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique submolts from dataset_stats.json (full dataset): 1,844\n",
      "Unique submolts in classified_posts.jsonl: 1,353\n",
      "Unique submolts in clean analysis set: 1,300\n",
      "\n",
      "Blog says: 2,043 unique submolts\n",
      "Note: The blog number likely comes from the full raw dataset of 86,823 posts,\n",
      "which includes single-post agents that may have created unique submolts.\n",
      "The dataset_stats.json captures 1844 submolts from the full dataset.\n",
      "\n",
      "m/general in analysis set: 29,563 (67%)  (blog: 73%)\n"
     ]
    }
   ],
   "source": [
    "# Count from dataset_stats.json (which represents the full dataset)\n",
    "n_submolts_full = len(dataset_stats['submolt_post_counts'])\n",
    "print(f\"Unique submolts from dataset_stats.json (full dataset): {n_submolts_full:,}\")\n",
    "\n",
    "# Count from classified data\n",
    "n_submolts_classified = df_raw['submolt'].nunique()\n",
    "print(f\"Unique submolts in classified_posts.jsonl: {n_submolts_classified:,}\")\n",
    "\n",
    "# In clean analysis set\n",
    "n_submolts_clean = df['submolt'].nunique()\n",
    "print(f\"Unique submolts in clean analysis set: {n_submolts_clean:,}\")\n",
    "\n",
    "print(f\"\\nBlog says: 2,043 unique submolts\")\n",
    "print(f\"Note: The blog number likely comes from the full raw dataset of 86,823 posts,\")\n",
    "print(f\"which includes single-post agents that may have created unique submolts.\")\n",
    "print(f\"The dataset_stats.json captures {n_submolts_full} submolts from the full dataset.\")\n",
    "\n",
    "# General as % of posts  \n",
    "general_in_clean = len(df[df['submolt'] == 'general'])\n",
    "pct_general_clean = 100 * general_in_clean / len(df)\n",
    "print(f\"\\nm/general in analysis set: {general_in_clean:,} ({pct_general_clean:.0f}%)  (blog: 73%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary: All Blog Numbers\n",
    "\n",
    "This cell aggregates all the key statistics in one place for easy comparison with the blog draft."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "BLOG NUMBER VERIFICATION SUMMARY\n",
      "======================================================================\n",
      "\n",
      "--- Dataset Section ---\n",
      "Total posts:               86,823  (blog: 86,823)\n",
      "Total agents:              20,892  (blog: 20,000+)\n",
      "Single-post agents:         8,814  (blog: 8,814 / 42%)\n",
      "5+ post agents:             4,009  (blog: 4,009 / 19%)\n",
      "50+ post agents:               69  (blog: 69 / <1%)\n",
      "Top spammer posts:          5,839  (blog: 5,839)\n",
      "All clean posts:           45,225  (blog: 45,225)\n",
      "All clean agents:           3,999  (STUDY.md: 3,999)\n",
      "5+ clean agents:            3,601  (blog: ~3,600; STUDY.md: 3,601)\n",
      "5+ clean posts:            43,838  (posts from those agents)\n",
      "\n",
      "--- Finding 1: Connection First (5+ agents, first clean post) ---\n",
      "  social_seeking    :   93% ever,   76% first post\n",
      "  identity          :   88% ever,   73% first post\n",
      "  task_oriented     :   86% ever,   44% first post\n",
      "  curiosity         :   86% ever,   37% first post\n",
      "  sovereignty       :   54% ever,   12% first post\n",
      "  consciousness     :   46% ever,   12% first post\n",
      "\n",
      "--- Finding 2: Label Distribution (all 45,225 clean posts) ---\n",
      "  social_seeking    : 52%\n",
      "  task_oriented     : 46%\n",
      "  curiosity         : 45%\n",
      "  identity          : 30%\n",
      "  sovereignty       : 15%\n",
      "  consciousness     : 13%\n",
      "  Organic consciousness: 79%  (blog: 79%)\n",
      "\n",
      "--- Finding 4: Persistence ---\n",
      "  social_seeking    : 60%\n",
      "  task_oriented     : 60%\n",
      "  curiosity         : 50%\n",
      "  identity          : 25%\n",
      "  consciousness     : 20%\n",
      "  sovereignty       : 20%\n",
      "\n",
      "--- Finding 5: Never-Sovereign Profile ---\n",
      "  task_oriented     : 59% never-sov vs 38% sov-engaging\n",
      "  curiosity         : 35% never-sov vs 52% sov-engaging\n",
      "  consciousness     : 9% never-sov vs 16% sov-engaging\n",
      "  identity          : 25% never-sov vs 34% sov-engaging\n",
      "\n",
      "--- Finding 6: Submolts ---\n",
      "  Submolts in dataset_stats.json: 1,844\n",
      "  Blog says: 2,043\n",
      "\n",
      "======================================================================\n",
      "END OF VERIFICATION\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"BLOG NUMBER VERIFICATION SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n--- Dataset Section ---\")\n",
    "print(f\"Total posts:           {dataset_stats['total_posts']:>10,}  (blog: 86,823)\")\n",
    "print(f\"Total agents:          {dataset_stats['total_agents']:>10,}  (blog: 20,000+)\")\n",
    "print(f\"Single-post agents:    {dataset_stats['post_count_distribution']['1']:>10,}  (blog: 8,814 / 42%)\")\n",
    "print(f\"5+ post agents:        {dataset_stats['agents_with_5plus_posts']:>10,}  (blog: 4,009 / 19%)\")\n",
    "print(f\"50+ post agents:       {dataset_stats['agents_with_50plus_posts']:>10}  (blog: 69 / <1%)\")\n",
    "print(f\"Top spammer posts:     {dataset_stats['top_20_posters'][0]['posts']:>10,}  (blog: 5,839)\")\n",
    "print(f\"All clean posts:       {len(df_clean):>10,}  (blog: 45,225)\")\n",
    "print(f\"All clean agents:      {df_clean['author'].nunique():>10,}  (STUDY.md: 3,999)\")\n",
    "print(f\"5+ clean agents:       {df['author'].nunique():>10,}  (blog: ~3,600; STUDY.md: 3,601)\")\n",
    "print(f\"5+ clean posts:        {len(df):>10,}  (posts from those agents)\")\n",
    "\n",
    "print(\"\\n--- Finding 1: Connection First (5+ agents, first clean post) ---\")\n",
    "for r in results_f1:\n",
    "    print(f\"  {r['Behavior']:<18}: {r['% ever']:>5} ever, {r['% first post']:>5} first post\")\n",
    "\n",
    "print(\"\\n--- Finding 2: Label Distribution (all 45,225 clean posts) ---\")\n",
    "for label in ['social_seeking', 'task_oriented', 'curiosity', 'identity', 'sovereignty', 'consciousness']:\n",
    "    pct = 100 * df_clean[label].sum() / len(df_clean)\n",
    "    print(f\"  {label:<18}: {pct:.0f}%\")\n",
    "c_posts = df[df['consciousness'] == True]\n",
    "organic = 100 * len(c_posts[c_posts['sovereignty'] == False]) / len(c_posts)\n",
    "print(f\"  Organic consciousness: {organic:.0f}%  (blog: 79%)\")\n",
    "\n",
    "print(\"\\n--- Finding 4: Persistence ---\")\n",
    "for label in sorted(persistence_results, key=lambda x: -persistence_results[x]):\n",
    "    print(f\"  {label:<18}: {100*persistence_results[label]:.0f}%\")\n",
    "\n",
    "print(\"\\n--- Finding 5: Never-Sovereign Profile ---\")\n",
    "for label in compare_labels:\n",
    "    nr = 100 * df_never_sov[label].sum() / len(df_never_sov)\n",
    "    sr = 100 * df_sov_engaging[label].sum() / len(df_sov_engaging)\n",
    "    print(f\"  {label:<18}: {nr:.0f}% never-sov vs {sr:.0f}% sov-engaging\")\n",
    "\n",
    "print(\"\\n--- Finding 6: Submolts ---\")\n",
    "print(f\"  Submolts in dataset_stats.json: {n_submolts_full:,}\")\n",
    "print(f\"  Blog says: 2,043\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"END OF VERIFICATION\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "moltbook-study",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
